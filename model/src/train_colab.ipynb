{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task PII Detection and Co-reference Detection Training\n",
    "\n",
    "This notebook trains a BERT-like model for two tasks:\n",
    "1. **PII Detection**: Identifying Personally Identifiable Information in text\n",
    "2. **Co-reference Detection**: Identifying mentions that refer to the same entity\n",
    "\n",
    "## Features\n",
    "- Multi-task learning with shared encoder and separate classification heads\n",
    "- Comprehensive metrics: Precision, Recall, F1 (weighted, macro, per-class)\n",
    "- **Apple Silicon (M4) GPU support** via Metal Performance Shaders (MPS)\n",
    "- Automatic device detection (MPS > CUDA > CPU)\n",
    "- Support for custom loss functions and class weights\n",
    "\n",
    "## Requirements\n",
    "- PyTorch with MPS support (for M4 Mac): `pip install torch torchvision torchaudio`\n",
    "- Training data in `model/dataset/training_samples/` directory\n",
    "- Python packages: transformers, datasets, scikit-learn, accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation (Local M4 Mac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# Uncomment the line below if you need to install packages\n",
    "# %pip install -q transformers datasets scikit-learn torch accelerate\n",
    "\n",
    "# For M4 Mac, make sure you have PyTorch with MPS support:\n",
    "# pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Training Data\n",
    "\n",
    "Make sure your training samples are in `model/dataset/training_samples/` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Checking for training data...\n",
      "   Project root: /Users/hannes/opensource/kiji-proxy\n",
      "   Training samples dir: /Users/hannes/opensource/kiji-proxy/dataset/training_samples\n",
      "‚úÖ Found training samples directory\n",
      "   Number of JSON files: 5\n",
      "   Sample files: [PosixPath('/Users/hannes/opensource/kiji-proxy/dataset/training_samples/20251124103832_fb0dd1a3caa8842bb2a1c9af9bbf3592e9c98d2545f4224d93c176e0e9ba7612.json'), PosixPath('/Users/hannes/opensource/kiji-proxy/dataset/training_samples/20251124103840_8dc19700e93885415fa096ea64e4717a6ec6c474a2eb14efe55d32d88226a158.json'), PosixPath('/Users/hannes/opensource/kiji-proxy/dataset/training_samples/20251124103848_29a096d927d7e4ee857641892e54733d3e7fd03c08dbaf454bd6b52845bbd532.json')]\n",
      "\n",
      "‚úÖ Training data ready!\n"
     ]
    }
   ],
   "source": [
    "# Verify local training data directory exists\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (assuming notebook is in model/ directory)\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"model\" else Path.cwd()\n",
    "training_samples_dir = project_root / \"model\" / \"dataset\" / \"training_samples\"\n",
    "\n",
    "print(\"üìÅ Checking for training data...\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   Training samples dir: {training_samples_dir}\")\n",
    "\n",
    "if training_samples_dir.exists():\n",
    "    json_files = list(training_samples_dir.glob(\"*.json\"))\n",
    "    print(\"‚úÖ Found training samples directory\")\n",
    "    print(f\"   Number of JSON files: {len(json_files)}\")\n",
    "    if len(json_files) > 0:\n",
    "        print(f\"   Sample files: {json_files[:3]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: No JSON files found in {training_samples_dir}\")\n",
    "        print(\"   Make sure your training samples are in this directory\")\n",
    "else:\n",
    "    print(f\"‚ùå Training samples directory not found: {training_samples_dir}\")\n",
    "    print(\"   Please ensure the model/dataset/training_samples directory exists\")\n",
    "    print(\"   and contains your training JSON files\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"Training samples directory not found: {training_samples_dir}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ Training data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Current directory: /Users/hannes/opensource/kiji-proxy/model\n",
      "üìÅ Project root: /Users/hannes/opensource/kiji-proxy\n",
      "üìÅ Python path includes: ['/Users/hannes/opensource/kiji-proxy/model', '/Users/hannes/.local/share/uv/python/cpython-3.13.9-macos-aarch64-none/lib/python313.zip', '/Users/hannes/.local/share/uv/python/cpython-3.13.9-macos-aarch64-none/lib/python3.13']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannes/opensource/kiji-proxy/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful (relative from model directory)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Determine project root - handle both notebook execution from model/ and project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"model\":\n",
    "    # Running from model/ directory\n",
    "    project_root = current_dir.parent\n",
    "    # Add both project root and model directory to path\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    if str(current_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(current_dir))\n",
    "else:\n",
    "    # Running from project root\n",
    "    project_root = current_dir\n",
    "    model_dir = current_dir / \"model\"\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    if str(model_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(model_dir))\n",
    "\n",
    "print(f\"üìÅ Current directory: {current_dir}\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üìÅ Python path includes: {list(sys.path[:3])}\")\n",
    "\n",
    "# Import training modules - try multiple import strategies\n",
    "try:\n",
    "    # Try absolute import from project root\n",
    "    from model.config import EnvironmentSetup, TrainingConfig\n",
    "    from model.preprocessing import DatasetProcessor\n",
    "    from model.trainer import PIITrainer\n",
    "\n",
    "    print(\"‚úÖ Imports successful (absolute from project root)\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try relative import (when running from model/ directory)\n",
    "        from config import EnvironmentSetup, TrainingConfig\n",
    "        from preprocessing import DatasetProcessor\n",
    "        from trainer import PIITrainer\n",
    "\n",
    "        print(\"‚úÖ Imports successful (relative from model directory)\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Import failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:49,411 - INFO - \n",
      "üìã Training Configuration:\n",
      "2025-11-24 14:23:49,412 - INFO -   Model: distilbert-base-cased\n",
      "2025-11-24 14:23:49,412 - INFO -   Epochs: 3\n",
      "2025-11-24 14:23:49,412 - INFO -   Batch Size: 32\n",
      "2025-11-24 14:23:49,412 - INFO -   Learning Rate: 3e-05\n",
      "2025-11-24 14:23:49,413 - INFO -   Max Samples: 400000\n",
      "2025-11-24 14:23:49,413 - INFO -   Output Dir: /Users/hannes/opensource/kiji-proxy/pii_model\n",
      "2025-11-24 14:23:49,413 - INFO -   Custom Loss: True\n"
     ]
    }
   ],
   "source": [
    "# Configure training parameters\n",
    "# Get project root for proper path resolution\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"model\" else Path.cwd()\n",
    "\n",
    "config = TrainingConfig(\n",
    "    # Model settings\n",
    "    model_name=\"distilbert-base-cased\",  # or \"bert-base-cased\", \"roberta-base\", etc.\n",
    "    # Training parameters\n",
    "    num_epochs=3,\n",
    "    batch_size=32,  # Adjust based on available memory (M4 Mac can handle this)\n",
    "    learning_rate=3e-5,\n",
    "    # Training optimization\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    # Output settings\n",
    "    output_dir=str(project_root / \"model\" / \"trained\"),  # Save in model directory\n",
    "    use_wandb=False,  # Set to True if using Weights & Biases\n",
    "    use_custom_loss=True,\n",
    "    # Dataset settings\n",
    "    eval_size_ratio=0.2,  # 20% for validation\n",
    "    training_samples_dir=str(\n",
    "        project_root / \"model\" / \"dataset\" / \"training_samples\"\n",
    "    ),  # Local path\n",
    "    # Multi-task learning weights\n",
    "    pii_loss_weight=1.0,\n",
    "    coref_loss_weight=1.0,\n",
    ")\n",
    "\n",
    "# Print configuration summary\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:49,417 - INFO - ‚úÖ Weights & Biases (wandb) disabled\n",
      "2025-11-24 14:23:49,430 - INFO - \n",
      "‚úÖ MPS (Metal) available: True\n",
      "2025-11-24 14:23:49,430 - INFO -    Using Apple Silicon GPU acceleration\n",
      "2025-11-24 14:23:49,430 - INFO -    Device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è  Training will use device: mps\n",
      "   ‚úÖ Using Apple Silicon GPU acceleration (Metal Performance Shaders)\n"
     ]
    }
   ],
   "source": [
    "# Disable wandb (if not using it)\n",
    "EnvironmentSetup.disable_wandb()\n",
    "\n",
    "# Check device availability (MPS for M4 Mac, CUDA for NVIDIA, or CPU)\n",
    "EnvironmentSetup.check_gpu()\n",
    "\n",
    "# Get the device that will be used for training\n",
    "device = EnvironmentSetup.get_device()\n",
    "print(f\"\\nüñ•Ô∏è  Training will use device: {device}\")\n",
    "if device.type == \"mps\":\n",
    "    print(\"   ‚úÖ Using Apple Silicon GPU acceleration (Metal Performance Shaders)\")\n",
    "elif device.type == \"cuda\":\n",
    "    print(\"   ‚úÖ Using NVIDIA GPU acceleration\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Using CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:49,434 - INFO - \n",
      "üì• Preparing datasets...\n",
      "2025-11-24 14:23:49,857 - INFO - \n",
      "üì• Loading training samples from /Users/hannes/opensource/kiji-proxy/dataset/training_samples...\n",
      "2025-11-24 14:23:49,857 - INFO - Found 5 JSON files\n",
      "2025-11-24 14:23:49,861 - INFO - ‚úÖ Loaded 5 training samples\n",
      "2025-11-24 14:23:49,951 - INFO - ‚úÖ Label mappings saved to /Users/hannes/opensource/kiji-proxy/pii_model/label_mappings.json\n",
      "2025-11-24 14:23:49,952 - INFO - \n",
      "üìä Dataset Summary:\n",
      "2025-11-24 14:23:49,952 - INFO -   Training samples: 4\n",
      "2025-11-24 14:23:49,952 - INFO -   Validation samples: 1\n",
      "2025-11-24 14:23:49,952 - INFO -   PII labels: 49\n",
      "2025-11-24 14:23:49,952 - INFO -   Co-reference labels: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Datasets prepared:\n",
      "  Training samples: 4\n",
      "  Validation samples: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset processor\n",
    "logger.info(\"\\nüì• Preparing datasets...\")\n",
    "dataset_processor = DatasetProcessor(config)\n",
    "\n",
    "# Load and prepare training/validation datasets\n",
    "train_dataset, val_dataset, mappings, coref_info = dataset_processor.prepare_datasets()\n",
    "\n",
    "print(\"\\n‚úÖ Datasets prepared:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:49,957 - INFO - \n",
      "üîß Initializing trainer...\n",
      "2025-11-24 14:23:50,320 - INFO - ‚úÖ Loaded 49 PII label mappings\n",
      "2025-11-24 14:23:50,320 - INFO - ‚úÖ Loaded 4 co-reference label mappings\n",
      "2025-11-24 14:23:50,462 - INFO - ‚úÖ Initialized multi-task loss (PII: 49 classes, Co-ref: 4 classes)\n",
      "2025-11-24 14:23:50,462 - INFO - ‚úÖ Model initialized with 49 PII labels and 4 co-reference labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model and trainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "logger.info(\"\\nüîß Initializing trainer...\")\n",
    "trainer = PIITrainer(config)\n",
    "\n",
    "# Load label mappings\n",
    "trainer.load_label_mappings(mappings, coref_info)\n",
    "\n",
    "# Initialize model\n",
    "trainer.initialize_model()\n",
    "\n",
    "print(\"\\n‚úÖ Model and trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:50,469 - INFO - \n",
      "üèãÔ∏è  Starting training...\n",
      "2025-11-24 14:23:50,469 - INFO - ============================================================\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "2025-11-24 14:23:50,561 - INFO - ‚úÖ Using MultiTaskTrainer with multi-task loss\n",
      "2025-11-24 14:23:50,561 - INFO - \n",
      "üèãÔ∏è  Starting multi-task training...\n",
      "2025-11-24 14:23:50,562 - INFO - ============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:53,386 - INFO - \n",
      "‚úÖ Training completed. Model saved to /Users/hannes/opensource/kiji-proxy/pii_model\n",
      "2025-11-24 14:23:53,386 - INFO - \n",
      "‚è±Ô∏è  Training completed in 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "logger.info(\"\\nüèãÔ∏è  Starting training...\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "trained_trainer = trainer.train(train_dataset, val_dataset)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "logger.info(f\"\\n‚è±Ô∏è  Training completed in {training_time / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:53,391 - INFO - \n",
      "üìä Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 14:23:53,711 - INFO - \n",
      "üìä Evaluation Results:\n",
      "2025-11-24 14:23:53,711 - INFO - \n",
      "üîç PII Detection Metrics:\n",
      "2025-11-24 14:23:53,711 - INFO -   F1:\n",
      "2025-11-24 14:23:53,711 - INFO -     eval_pii_f1: 0.0143\n",
      "2025-11-24 14:23:53,712 - INFO -     eval_pii_f1_macro: 0.0174\n",
      "2025-11-24 14:23:53,712 - INFO -     eval_pii_f1_weighted: 0.0143\n",
      "2025-11-24 14:23:53,712 - INFO -   PRECISION:\n",
      "2025-11-24 14:23:53,712 - INFO -     eval_pii_precision_macro: 0.0145\n",
      "2025-11-24 14:23:53,712 - INFO -     eval_pii_precision_weighted: 0.0119\n",
      "2025-11-24 14:23:53,713 - INFO -   RECALL:\n",
      "2025-11-24 14:23:53,713 - INFO -     eval_pii_recall_macro: 0.0217\n",
      "2025-11-24 14:23:53,713 - INFO -     eval_pii_recall_weighted: 0.0179\n",
      "2025-11-24 14:23:53,713 - INFO - \n",
      "üîç Co-reference Detection Metrics:\n",
      "2025-11-24 14:23:53,713 - INFO -   F1:\n",
      "2025-11-24 14:23:53,713 - INFO -     eval_coref_f1: 0.0785\n",
      "2025-11-24 14:23:53,714 - INFO -     eval_coref_f1_macro: 0.0505\n",
      "2025-11-24 14:23:53,714 - INFO -     eval_coref_f1_weighted: 0.0785\n",
      "2025-11-24 14:23:53,714 - INFO -   PRECISION:\n",
      "2025-11-24 14:23:53,714 - INFO -     eval_coref_precision_macro: 0.1852\n",
      "2025-11-24 14:23:53,714 - INFO -     eval_coref_precision_weighted: 0.4206\n",
      "2025-11-24 14:23:53,715 - INFO -   RECALL:\n",
      "2025-11-24 14:23:53,715 - INFO -     eval_coref_recall_macro: 0.0478\n",
      "2025-11-24 14:23:53,715 - INFO -     eval_coref_recall_weighted: 0.0536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üîç PII Detection Metrics:\n",
      "  F1 (weighted): 0.0143\n",
      "  F1 (macro): 0.0174\n",
      "  Precision (weighted): 0.0119\n",
      "  Precision (macro): 0.0145\n",
      "  Recall (weighted): 0.0179\n",
      "  Recall (macro): 0.0217\n",
      "\n",
      "üîç Co-reference Detection Metrics:\n",
      "  F1 (weighted): 0.0785\n",
      "  F1 (macro): 0.0505\n",
      "  Precision (weighted): 0.4206\n",
      "  Precision (macro): 0.1852\n",
      "  Recall (weighted): 0.0536\n",
      "  Recall (macro): 0.0478\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "logger.info(\"\\nüìä Evaluating model...\")\n",
    "results = trainer.evaluate(val_dataset, trained_trainer)\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüîç PII Detection Metrics:\")\n",
    "print(f\"  F1 (weighted): {results.get('eval_pii_f1_weighted', 'N/A'):.4f}\")\n",
    "print(f\"  F1 (macro): {results.get('eval_pii_f1_macro', 'N/A'):.4f}\")\n",
    "print(\n",
    "    f\"  Precision (weighted): {results.get('eval_pii_precision_weighted', 'N/A'):.4f}\"\n",
    ")\n",
    "print(f\"  Precision (macro): {results.get('eval_pii_precision_macro', 'N/A'):.4f}\")\n",
    "print(f\"  Recall (weighted): {results.get('eval_pii_recall_weighted', 'N/A'):.4f}\")\n",
    "print(f\"  Recall (macro): {results.get('eval_pii_recall_macro', 'N/A'):.4f}\")\n",
    "\n",
    "if \"eval_coref_f1_weighted\" in results:\n",
    "    print(\"\\nüîç Co-reference Detection Metrics:\")\n",
    "    print(f\"  F1 (weighted): {results.get('eval_coref_f1_weighted', 'N/A'):.4f}\")\n",
    "    print(f\"  F1 (macro): {results.get('eval_coref_f1_macro', 'N/A'):.4f}\")\n",
    "    print(\n",
    "        f\"  Precision (weighted): {results.get('eval_coref_precision_weighted', 'N/A'):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Precision (macro): {results.get('eval_coref_precision_macro', 'N/A'):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Recall (weighted): {results.get('eval_coref_recall_weighted', 'N/A'):.4f}\"\n",
    "    )\n",
    "    print(f\"  Recall (macro): {results.get('eval_coref_recall_macro', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Saved Locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Model saved locally at: /Users/hannes/opensource/kiji-proxy/pii_model\n",
      "\n",
      "üìÅ Model files:\n",
      "   README.md (0.01 MB)\n",
      "   label_mappings.json (0.00 MB)\n",
      "   model.safetensors (248.85 MB)\n",
      "   special_tokens_map.json (0.00 MB)\n",
      "   tokenizer.json (0.64 MB)\n",
      "   tokenizer_config.json (0.00 MB)\n",
      "   training_args.bin (0.01 MB)\n",
      "   vocab.txt (0.20 MB)\n",
      "\n",
      "‚úÖ Model is ready to use!\n",
      "üí° You can load it using:\n",
      "   from model.model import MultiTaskPIIDetectionModel\n",
      "   model = MultiTaskPIIDetectionModel.from_pretrained('/Users/hannes/opensource/kiji-proxy/pii_model')\n"
     ]
    }
   ],
   "source": [
    "# Model saved locally\n",
    "model_path = Path(config.output_dir).absolute()\n",
    "print(f\"\\nüíæ Model saved locally at: {model_path}\")\n",
    "\n",
    "# List model files\n",
    "if model_path.exists():\n",
    "    model_files = list(model_path.glob(\"*\"))\n",
    "    print(\"\\nüìÅ Model files:\")\n",
    "    for f in sorted(model_files):\n",
    "        if f.is_file():\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {f.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "    print(\"\\n‚úÖ Model is ready to use!\")\n",
    "    print(\"üí° You can load it using:\")\n",
    "    print(\"   from model.model import MultiTaskPIIDetectionModel\")\n",
    "    print(f\"   model = MultiTaskPIIDetectionModel.from_pretrained('{model_path}')\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Model directory not found: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä Final Metrics:\n",
      "  PII F1 (weighted): 0.0143\n",
      "  Co-reference F1 (weighted): 0.0785\n",
      "\n",
      "üíæ Model saved to: /Users/hannes/opensource/kiji-proxy/pii_model\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìä Final Metrics:\")\n",
    "print(\n",
    "    f\"  PII F1 (weighted): {results.get('eval_pii_f1_weighted', results.get('eval_pii_f1', 'N/A')):.4f}\"\n",
    ")\n",
    "if \"eval_coref_f1_weighted\" in results:\n",
    "    print(\n",
    "        f\"  Co-reference F1 (weighted): {results.get('eval_coref_f1_weighted', 'N/A'):.4f}\"\n",
    "    )\n",
    "print(f\"\\nüíæ Model saved to: {config.output_dir}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Download the model**: The trained model is saved in `config.output_dir`\n",
    "2. **Use the model**: Load it using `MultiTaskPIIDetectionModel.from_pretrained()`\n",
    "3. **Evaluate on test set**: Use the `evaluate()` method with your test dataset\n",
    "4. **Fine-tune**: Adjust hyperparameters and retrain if needed\n",
    "\n",
    "### Model Files\n",
    "- `pytorch_model.bin`: Model weights\n",
    "- `config.json`: Model configuration\n",
    "- `label_mappings.json`: Label mappings for both tasks\n",
    "- `tokenizer_config.json` and related files: Tokenizer configuration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
